# Copy to .env and fill in your values

# ── LLM Provider ──────────────────────────────────────────────────────────────
LLM_PROVIDER=anthropic           # anthropic | openai | groq | ollama | deepseek
LLM_MODEL=claude-haiku-4-5-20251001

# API Keys
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...
GROQ_API_KEY=gsk_...

# Custom base URL (for Ollama, LM Studio, Azure, proxies)
# LLM_BASE_URL=http://localhost:11434/v1

# ── Agent Behaviour ───────────────────────────────────────────────────────────
AGENT_MAX_ITER=15
AGENT_TEMPERATURE=0.0
AGENT_MAX_TOKENS=4096

# ── Memory ────────────────────────────────────────────────────────────────────
MEMORY_DIR=.agent_memory
MEMORY_WINDOW=40

# ── Reflection ────────────────────────────────────────────────────────────────
ENABLE_REFLECTION=false          # Set to true for high-stakes tasks
REFLECTION_RETRIES=2
